---
title: "Random Seeds"
author: "GHD"
date: "28/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## What is a random seed?

Computes tend not to use truly random numbers. Instead they use algorithms called pseudorandom number generators (PRNGs) to produce 'pseudorandom' numbers.

PRNGs are deterministic algorithms which produce sequences of numbers which have many of the same statistical properties as truly random sequences. 
They tend to work iteratively, the algorithm has a 'hidden state' which acts as an input to a complex transformation to produce an output, each time this occurs the hidden state is updated so that at the next step the algorithm will produce a different output.

Because they are deterministic if the algorithm is in same state and receives the same input it will produce the same output. This can be a very useful property as often we would like to write some code which uses random numbers but we want to produce the same output every time we run it.

The *seed* of a PRNG is a way of determining the starting value of the hidden state. Ordinarily these algorithms will take their seed from a source which is constantly changing, such as the computers clock time, which ensures that each time you produce random numbers they will be different. However we have the option to manually provide a seed to set the initial state.

In R this is done with `set.seed(n)`.

## Simulations


If we simulate and visualise two sets of gaussian data we can see that they are different as expected.
```{r simulate-norm-1}
# Simulate 
set.seed(1)
x1 <- rnorm(10)
x2 <- rnorm(10)

# Plot
par(mfrow=c(1,2))
hist(x1)
hist(x2)
```
This is because after producing the first set of data our PRNG is no longer in the state specified by `set.seed(1)` and so the second dataset will be different.

If however we reseed the algorithm after producing the first dataset we will produce exactly the same set of pseudorandom numbers:

```{r simulate-norm-2}
# Set random seed to 1.
set.seed(1)
x1 <- rnorm(10)

# Reset the random seed to 1.
set.seed(1)
x2 <- rnorm(10)

# Plot
par(mfrow=c(1,2))
hist(x1)
hist(x2)
```

Note the changes in the state of the PRNG only occur when it is used to produce our pseudorandom numbers. Therefore producing 10 gaussian samples with `rnorm(10)` is equivalent to sequentially producing two sets of 5 samples with `rnorm(5)` and combining them: 

```{r}
# Sample all 10 in one command.
set.seed(1)
x1 <- rnorm(10)

# Sample two sets of 5 sequentially.
set.seed(1)
y1 <- rnorm(5)
y2 <- rnorm(5)
x2 <- c(y1,y2)
par(mfrow=c(1,2))
hist(x1)
hist(x2)
```
It doesn't matter what code was used to produce the random numbers, what is important is the state of the PRNG 'under the hood'.

This can lead to some slightly unexpected behaviour which we will briefly look at below.

First lets simulate a sequence of numbers from a uniform distribution between 0 and 1, $U(0,1)$:
 
```{r}
set.seed(1)
runif(5)
```

When we precede this operation by simulating a single uniformly distributed number it will shift our sequence one forward, the second element moving to the first, the third to the second and so on, with a new number taking the 5th position:
```{r}
set.seed(1)
old <- runif(5)
set.seed(1)
ignore <- runif(1)
new <- runif(5)

knitr::kable(data.frame(old,new),digits=3,row.names=TRUE)
```

```{r}
set.seed(1)
seq1 <- runif(5)
set.seed(1)
ignore <- runif(1)
seq2 <- runif(5)

df <- data.frame(Seq1=c(old,NA),Seq2=c(NA,new))
knitr::kable(df,digits=3,row.names=TRUE)
```

If we peek at the number we discarded we will see that, as we might of expected, it corresponds to the first element of our original sequence:

```{r}
ignore
```

So far so good.

However what will happen if, instead of a uniformly distribution variable, we simulate from a gaussian?

```{r}
set.seed(1)
seq1 <- runif(5)
set.seed(1)
ignore <- rnorm(1)
seq2 <- runif(5)

df <- data.frame(Seq1=c(seq1,rep(NA,2)), Seq2=c(rep(NA,2),seq2))
knitr::kable(df,digits=3,row.names=TRUE)
```

This time our samples have been shifted two positions forward!

What is going on?

The answer is a little out of the scope of this tutorial but the key piece of information is that a normally distributed random variable from $N(0,1)$ is "more random" than a uniformly distributed variable from $U(0,1)$. Simulating a single gaussian variable is equivalent to simulating two uniform variables, our PRNG must be used twice and so the hidden state is advanced two times. 

This is important as it means that in practice it is very hard to keep track of the internal state of the PRNG. Say you have two analysis scripts and you want some key algorithms inside to use the same pseudorandom numbers (for example you might want your tSNE plots to be reproducible). It is not sufficient to set the same random seed at the top of the script as it is all to easy to inadvertantly create a divergence in the hidden state of the PRNG between the two scripts. Therefore it is best to reseed the random number generator just the relevant code and to ensure that any intervening steps are identical.






