---
title: "Clustering"
author: "GHD"
date: "12/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message=FALSE}
library(scater)
library(scran)
```

Large sections of this document were inspired by the [Orchestrating Singe-Cell Analysis](https://osca.bioconductor.org/clustering.html) online book.

## Load Data
We will use the peripheral blood mononuclear cell (PBMC) dataset from 10X Genomics (Zheng et al. 2017).

For simplicity we will load the data as a `SingleCellExperiment` object from a saved `.rds` file. This data has already had some basic QC, normalisation, and dimensionality reduction applied.

```{r load_data}
sce.pbmc <- readRDS("pbmc_preproc.rds")
```

## Clustering

The most important thing to remember about clustering is that by choosing the right algorithm and representation of our data we can get any groupings we please. Clustering is just one of many tools we have to explore data and the perspectives it offers are only as reasonable as the choices and assumptions that underlie them.

## k-means

There are many good [explanations](https://blog.easysol.net/machine-learning-algorithms-3) and [visualisations](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) of how the k-means algorithm works online so I won't go into much detail here. The objective of the algorithm is to assign each point to a cluster in such a way that minimizes the sum of the squared distances between each point and the average position of all of the points which are assigned to its cluster.

```{r, fig.cap="t-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from k-means clustering."}
set.seed(100)
clust.kmeans <- kmeans(reducedDim(sce.pbmc, "PCA"), centers=10)
colLabels(sce.pbmc) <- factor(clust.kmeans$cluster)
plotReducedDim(sce.pbmc, "TSNE", colour_by="label") +
  ggtitle("k-mean Clustering")
```

### Choosing k

The key parameter in k-means clustering is choice of the number of clusters, k. By varying this parameter we can move from coarse-grained to more fine-grained clusters.

```{r, echo=FALSE}
set.seed(100)
k <- 3
clust.kmeans <- kmeans(reducedDim(sce.pbmc, "PCA"), centers=k)
colLabels(sce.pbmc) <- factor(clust.kmeans$cluster)
kmeans_3 <- plotReducedDim(sce.pbmc, "TSNE", colour_by="label",add_legend=FALSE) +
  ggtitle(paste(k, "Clusters"))

k <- 5
clust.kmeans <- kmeans(reducedDim(sce.pbmc, "PCA"), centers=k)
colLabels(sce.pbmc) <- factor(clust.kmeans$cluster)
kmeans_5 <- plotReducedDim(sce.pbmc, "TSNE", colour_by="label",add_legend=FALSE) +
  ggtitle(paste(k, "Clusters"))

k <- 10
clust.kmeans <- kmeans(reducedDim(sce.pbmc, "PCA"), centers=k)
colLabels(sce.pbmc) <- factor(clust.kmeans$cluster)
kmeans_10 <- plotReducedDim(sce.pbmc, "TSNE", colour_by="label",add_legend=FALSE) +
  ggtitle(paste(k, "Clusters"))

k <- 20
clust.kmeans <- kmeans(reducedDim(sce.pbmc, "PCA"), centers=k)
colLabels(sce.pbmc) <- factor(clust.kmeans$cluster)
kmeans_20 <- plotReducedDim(sce.pbmc, "TSNE", colour_by="label",add_legend=FALSE) +
  ggtitle(paste(k, "Clusters"))

multiplot(kmeans_3,kmeans_5,kmeans_10,kmeans_20,layout=matrix(seq(4),nrow=2,byrow=TRUE))
```

One use of k-means is to deliberately set k to a large value to "overcluster" the data. k-means will happily split up large clusters which graph-based methods try to keep intact, by examining how clusters divide into subclusters can provide some insight on the internal structure of these groups of cells. This is a nice example of the philosophy of using clustering as a tool to interrogate data, rather than a source of 'objective' cell populations.

So how do we choose a reasonable choice of k?

One possibility is to use a measure of how well our clusters fit the variation in the data. The gap statistic is the log-ratio of the expected to observed within-cluster sum of squares with larger values representing better clustering. Often instead of choosing the k with the largest gap statistic we will choose the lowest k with a statistic within some threshold of the best statistic we observe.

```{r, eval=FALSE, warning=FALSE}
library(cluster)
set.seed(110010101)
gaps <- clusGap(reducedDim(sce.pbmc, "PCA"), kmeans, K.max=15)
# Choose the smallest k for which the gap statistic is within 1 SE of the best k.
best.k <- maxSE(gaps$Tab[,"gap"], gaps$Tab[,"SE.sim"])

plot(gaps$Tab[,"gap"], xlab="Number of clusters", ylab="Gap statistic")
abline(v=best.k, col="red")
```
Gap statistic with respect to increasing number of k-means clusters in the 10X PBMC dataset. The red line represents the chosen k.

### Which supspace?

The k-means algorithm uses the distance between points to cluster them. which the distances between points and centers. However these distances depend on the space in which our points are lying, so which space should we choose?

We might choose the 'full-space' in which each dimension corresponds to the expression of a different gene. In the case of the pbmc data we have ~30,000 dimensions in our data. It can be quite computationally expensive to work in this full space. Instead we could perform on the data in a reduced dimensionality space such as PCA, $t$-SNE, or UMAP. Very low dimensional representations of the data such as 2D $t$-SNE or UMAP cannot include much of the true variation that exists in the data, if we clustered on them directly we would miss important structure in the data. Therefore usually a compromise is made and clustering is performed on the data in the subspace formed by the first 50-100 PCs from PCA, this hopefully has sufficiently many dimensions to contain most of the important structure in the data while not being too computationally expensive to work with.

After clustering in this higher dimensional space we use the lower dimension $t$-SNE or UMAP to plot the results.

Below we can see the difference between clustering in PCA space vs directly on the two $t$-SNE dimensions. Note how the mixing of cells with different labels in the PCA space plot contrasts with the clean cluster boundaries from $t$-SNE clustering. 

```{r which-subspace}
N_centers = 5
set.seed(100)
clust.kmeans.pca.space <- kmeans(reducedDim(sce.pbmc, "PCA"), centers=N_centers)
set.seed(100)
clust.kmeans.tsne.space <- kmeans(reducedDim(sce.pbmc, "TSNE"), centers=N_centers)

colLabels(sce.pbmc) <- factor(clust.kmeans.pca.space$cluster)
kmeansPlotPcaSpace <- plotReducedDim(sce.pbmc, "TSNE", colour_by="label") +
  ggtitle("Clustered in PCA space")
colLabels(sce.pbmc) <- factor(clust.kmeans.tsne.space$cluster)
kmeansPlotTsneSpace <- plotReducedDim(sce.pbmc, "TSNE", colour_by="label") +
  ggtitle("Clustered in tSNE space")

multiplot(kmeansPlotPcaSpace,kmeansPlotTsneSpace,cols=2)
```

## Graph-based clustering

There are several considerations in the practical execution of a graph-based clustering method:

- How many neighbors are considered when constructing the graph.
- What scheme is used to weight the edges.
- Which community detection algorithm is used to define the clusters.

For example, the following code uses the 10 nearest neighbors of each cell to construct a shared nearest neighbor graph. Two cells are connected by an edge if any of their nearest neighbors are shared, with the edge weight defined from the highest average rank of the shared neighbors (Xu and Su 2015). The Walktrap method from the igraph package is then used to identify communities. All calculations are performed using the top PCs to take advantage of data compression and denoising.

```{r}
library(scran)
g <- buildSNNGraph(sce.pbmc, k=10, use.dimred = 'PCA')
clust <- igraph::cluster_walktrap(g)$membership
table(clust)
```

We assign the cluster assignments back into our SingleCellExperiment object as a factor in the column metadata. This allows us to conveniently visualize the distribution of clusters in a  
t
 -SNE plot (Figure 10.1).

```{r}
library(scater)
colLabels(sce.pbmc) <- factor(clust)
plotReducedDim(sce.pbmc, "TSNE", colour_by="label")
```
$t$-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from graph-based clustering.
Figure 10.1:  
t
 -SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster from graph-based clustering.

One of the most important parameters is k, the number of nearest neighbors used to construct the graph. This controls the resolution of the clustering where higher k yields a more inter-connected graph and broader clusters. Users can exploit this by experimenting with different values of k to obtain a satisfactory resolution.

```{r}
# More resolved.
g.5 <- buildSNNGraph(sce.pbmc, k=5, use.dimred = 'PCA')
clust.5 <- igraph::cluster_walktrap(g.5)$membership
table(clust.5)
```
```{r}
# Less resolved.
g.50 <- buildSNNGraph(sce.pbmc, k=50, use.dimred = 'PCA')
clust.50 <- igraph::cluster_walktrap(g.50)$membership
table(clust.50)
```

The graph itself can be visualized using a force-directed layout (Figure 10.2). This yields a dimensionality reduction result that is closely related to  
$t$-SNE and UMAP, though which of these is the most aesthetically pleasing is left to the eye of the beholder.

```{r}
set.seed(2000)
reducedDim(sce.pbmc, "force") <- igraph::layout_with_fr(g)
plotReducedDim(sce.pbmc, colour_by="label", dimred="force")
```

## Bibliography
Zheng, G. X., J. M. Terry, P. Belgrader, P. Ryvkin, Z. W. Bent, R. Wilson, S. B. Ziraldo, et al. 2017. “Massively parallel digital transcriptional profiling of single cells.” Nat Commun 8 (January): 14049.